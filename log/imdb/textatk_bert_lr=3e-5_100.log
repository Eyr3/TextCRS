nohup: ignoring input
2023-01-16 03:51:45.143814: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-16 03:51:45.337981: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-01-16 03:51:45.954737: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-01-16 03:51:45.954825: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-01-16 03:51:45.954834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-16 03:51:46.940591: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2023-01-16 03:51:46.940648: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
textattacknew: Loading transformers AutoModelForSequenceClassification
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
textattacknew: Model state dict:
textattacknew: bert.embeddings.position_ids	torch.Size([1, 512])
textattacknew: bert.embeddings.word_embeddings.weight	torch.Size([30522, 768])
textattacknew: bert.embeddings.position_embeddings.weight	torch.Size([512, 768])
textattacknew: bert.embeddings.token_type_embeddings.weight	torch.Size([2, 768])
textattacknew: bert.embeddings.LayerNorm.weight	torch.Size([768])
textattacknew: bert.embeddings.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.0.attention.self.query.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.0.attention.self.query.bias	torch.Size([768])
textattacknew: bert.encoder.layer.0.attention.self.key.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.0.attention.self.key.bias	torch.Size([768])
textattacknew: bert.encoder.layer.0.attention.self.value.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.0.attention.self.value.bias	torch.Size([768])
textattacknew: bert.encoder.layer.0.attention.output.dense.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.0.attention.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.0.attention.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.0.attention.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.0.intermediate.dense.weight	torch.Size([3072, 768])
textattacknew: bert.encoder.layer.0.intermediate.dense.bias	torch.Size([3072])
textattacknew: bert.encoder.layer.0.output.dense.weight	torch.Size([768, 3072])
textattacknew: bert.encoder.layer.0.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.0.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.0.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.1.attention.self.query.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.1.attention.self.query.bias	torch.Size([768])
textattacknew: bert.encoder.layer.1.attention.self.key.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.1.attention.self.key.bias	torch.Size([768])
textattacknew: bert.encoder.layer.1.attention.self.value.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.1.attention.self.value.bias	torch.Size([768])
textattacknew: bert.encoder.layer.1.attention.output.dense.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.1.attention.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.1.attention.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.1.attention.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.1.intermediate.dense.weight	torch.Size([3072, 768])
textattacknew: bert.encoder.layer.1.intermediate.dense.bias	torch.Size([3072])
textattacknew: bert.encoder.layer.1.output.dense.weight	torch.Size([768, 3072])
textattacknew: bert.encoder.layer.1.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.1.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.1.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.2.attention.self.query.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.2.attention.self.query.bias	torch.Size([768])
textattacknew: bert.encoder.layer.2.attention.self.key.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.2.attention.self.key.bias	torch.Size([768])
textattacknew: bert.encoder.layer.2.attention.self.value.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.2.attention.self.value.bias	torch.Size([768])
textattacknew: bert.encoder.layer.2.attention.output.dense.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.2.attention.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.2.attention.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.2.attention.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.2.intermediate.dense.weight	torch.Size([3072, 768])
textattacknew: bert.encoder.layer.2.intermediate.dense.bias	torch.Size([3072])
textattacknew: bert.encoder.layer.2.output.dense.weight	torch.Size([768, 3072])
textattacknew: bert.encoder.layer.2.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.2.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.2.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.3.attention.self.query.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.3.attention.self.query.bias	torch.Size([768])
textattacknew: bert.encoder.layer.3.attention.self.key.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.3.attention.self.key.bias	torch.Size([768])
textattacknew: bert.encoder.layer.3.attention.self.value.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.3.attention.self.value.bias	torch.Size([768])
textattacknew: bert.encoder.layer.3.attention.output.dense.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.3.attention.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.3.attention.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.3.attention.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.3.intermediate.dense.weight	torch.Size([3072, 768])
textattacknew: bert.encoder.layer.3.intermediate.dense.bias	torch.Size([3072])
textattacknew: bert.encoder.layer.3.output.dense.weight	torch.Size([768, 3072])
textattacknew: bert.encoder.layer.3.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.3.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.3.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.4.attention.self.query.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.4.attention.self.query.bias	torch.Size([768])
textattacknew: bert.encoder.layer.4.attention.self.key.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.4.attention.self.key.bias	torch.Size([768])
textattacknew: bert.encoder.layer.4.attention.self.value.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.4.attention.self.value.bias	torch.Size([768])
textattacknew: bert.encoder.layer.4.attention.output.dense.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.4.attention.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.4.attention.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.4.attention.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.4.intermediate.dense.weight	torch.Size([3072, 768])
textattacknew: bert.encoder.layer.4.intermediate.dense.bias	torch.Size([3072])
textattacknew: bert.encoder.layer.4.output.dense.weight	torch.Size([768, 3072])
textattacknew: bert.encoder.layer.4.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.4.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.4.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.5.attention.self.query.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.5.attention.self.query.bias	torch.Size([768])
textattacknew: bert.encoder.layer.5.attention.self.key.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.5.attention.self.key.bias	torch.Size([768])
textattacknew: bert.encoder.layer.5.attention.self.value.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.5.attention.self.value.bias	torch.Size([768])
textattacknew: bert.encoder.layer.5.attention.output.dense.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.5.attention.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.5.attention.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.5.attention.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.5.intermediate.dense.weight	torch.Size([3072, 768])
textattacknew: bert.encoder.layer.5.intermediate.dense.bias	torch.Size([3072])
textattacknew: bert.encoder.layer.5.output.dense.weight	torch.Size([768, 3072])
textattacknew: bert.encoder.layer.5.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.5.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.5.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.6.attention.self.query.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.6.attention.self.query.bias	torch.Size([768])
textattacknew: bert.encoder.layer.6.attention.self.key.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.6.attention.self.key.bias	torch.Size([768])
textattacknew: bert.encoder.layer.6.attention.self.value.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.6.attention.self.value.bias	torch.Size([768])
textattacknew: bert.encoder.layer.6.attention.output.dense.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.6.attention.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.6.attention.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.6.attention.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.6.intermediate.dense.weight	torch.Size([3072, 768])
textattacknew: bert.encoder.layer.6.intermediate.dense.bias	torch.Size([3072])
textattacknew: bert.encoder.layer.6.output.dense.weight	torch.Size([768, 3072])
textattacknew: bert.encoder.layer.6.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.6.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.6.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.7.attention.self.query.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.7.attention.self.query.bias	torch.Size([768])
textattacknew: bert.encoder.layer.7.attention.self.key.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.7.attention.self.key.bias	torch.Size([768])
textattacknew: bert.encoder.layer.7.attention.self.value.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.7.attention.self.value.bias	torch.Size([768])
textattacknew: bert.encoder.layer.7.attention.output.dense.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.7.attention.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.7.attention.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.7.attention.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.7.intermediate.dense.weight	torch.Size([3072, 768])
textattacknew: bert.encoder.layer.7.intermediate.dense.bias	torch.Size([3072])
textattacknew: bert.encoder.layer.7.output.dense.weight	torch.Size([768, 3072])
textattacknew: bert.encoder.layer.7.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.7.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.7.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.8.attention.self.query.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.8.attention.self.query.bias	torch.Size([768])
textattacknew: bert.encoder.layer.8.attention.self.key.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.8.attention.self.key.bias	torch.Size([768])
textattacknew: bert.encoder.layer.8.attention.self.value.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.8.attention.self.value.bias	torch.Size([768])
textattacknew: bert.encoder.layer.8.attention.output.dense.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.8.attention.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.8.attention.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.8.attention.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.8.intermediate.dense.weight	torch.Size([3072, 768])
textattacknew: bert.encoder.layer.8.intermediate.dense.bias	torch.Size([3072])
textattacknew: bert.encoder.layer.8.output.dense.weight	torch.Size([768, 3072])
textattacknew: bert.encoder.layer.8.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.8.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.8.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.9.attention.self.query.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.9.attention.self.query.bias	torch.Size([768])
textattacknew: bert.encoder.layer.9.attention.self.key.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.9.attention.self.key.bias	torch.Size([768])
textattacknew: bert.encoder.layer.9.attention.self.value.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.9.attention.self.value.bias	torch.Size([768])
textattacknew: bert.encoder.layer.9.attention.output.dense.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.9.attention.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.9.attention.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.9.attention.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.9.intermediate.dense.weight	torch.Size([3072, 768])
textattacknew: bert.encoder.layer.9.intermediate.dense.bias	torch.Size([3072])
textattacknew: bert.encoder.layer.9.output.dense.weight	torch.Size([768, 3072])
textattacknew: bert.encoder.layer.9.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.9.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.9.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.10.attention.self.query.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.10.attention.self.query.bias	torch.Size([768])
textattacknew: bert.encoder.layer.10.attention.self.key.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.10.attention.self.key.bias	torch.Size([768])
textattacknew: bert.encoder.layer.10.attention.self.value.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.10.attention.self.value.bias	torch.Size([768])
textattacknew: bert.encoder.layer.10.attention.output.dense.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.10.attention.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.10.attention.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.10.attention.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.10.intermediate.dense.weight	torch.Size([3072, 768])
textattacknew: bert.encoder.layer.10.intermediate.dense.bias	torch.Size([3072])
textattacknew: bert.encoder.layer.10.output.dense.weight	torch.Size([768, 3072])
textattacknew: bert.encoder.layer.10.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.10.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.10.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.11.attention.self.query.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.11.attention.self.query.bias	torch.Size([768])
textattacknew: bert.encoder.layer.11.attention.self.key.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.11.attention.self.key.bias	torch.Size([768])
textattacknew: bert.encoder.layer.11.attention.self.value.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.11.attention.self.value.bias	torch.Size([768])
textattacknew: bert.encoder.layer.11.attention.output.dense.weight	torch.Size([768, 768])
textattacknew: bert.encoder.layer.11.attention.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.11.attention.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.11.attention.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.encoder.layer.11.intermediate.dense.weight	torch.Size([3072, 768])
textattacknew: bert.encoder.layer.11.intermediate.dense.bias	torch.Size([3072])
textattacknew: bert.encoder.layer.11.output.dense.weight	torch.Size([768, 3072])
textattacknew: bert.encoder.layer.11.output.dense.bias	torch.Size([768])
textattacknew: bert.encoder.layer.11.output.LayerNorm.weight	torch.Size([768])
textattacknew: bert.encoder.layer.11.output.LayerNorm.bias	torch.Size([768])
textattacknew: bert.pooler.dense.weight	torch.Size([768, 768])
textattacknew: bert.pooler.dense.bias	torch.Size([768])
textattacknew: classifier.weight	torch.Size([2, 768])
textattacknew: classifier.bias	torch.Size([2])
textattacknew: Num params: 85646594
textattacknew: train set length: 20000
textattacknew: val set length: 5000
textattacknew: test set length: 25000
textattacknew: Writing logs to /data/xinyu/results/fgws/models/bert/imdb/textatk_lr=3e-05/train_log.txt.
textattacknew: Wrote original training args to /data/xinyu/results/fgws/models/bert/imdb/textatk_lr=3e-05/training_args.json.
/home/zhangxinyu/.conda/envs/torch19/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
textattacknew: ***** Running training *****
textattacknew:   Num examples = 20000
textattacknew:   Num epochs = 10
textattacknew:   Num clean epochs = 10
textattacknew:   Instantaneous batch size per device = 100
textattacknew:   Total train batch size (w. parallel, distributed & accumulation) = 100
textattacknew:   Gradient accumulation steps = 1
textattacknew:   Total optimization steps = 2000
textattacknew: ==========================================================
textattacknew: Epoch 1
textattacknew: Running clean epoch 1/10
Iteration:   0%|          | 0/200 [00:00<?, ?it/s]Iteration:   0%|          | 0/200 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/zhangxinyu/code/fgws-main/textatk_train.py", line 73, in <module>
    trainer.train()
  File "/home/zhangxinyu/code/fgws-main/textattacknew/trainer.py", line 735, in train
    loss, preds, targets = self.training_step(model, tokenizer, batch)
  File "/home/zhangxinyu/code/fgws-main/textattacknew/trainer.py", line 528, in training_step
    logits = model(**input_ids)[0]
  File "/home/zhangxinyu/.conda/envs/torch19/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zhangxinyu/.conda/envs/torch19/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1566, in forward
    outputs = self.bert(
  File "/home/zhangxinyu/.conda/envs/torch19/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zhangxinyu/.conda/envs/torch19/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1021, in forward
    encoder_outputs = self.encoder(
  File "/home/zhangxinyu/.conda/envs/torch19/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zhangxinyu/.conda/envs/torch19/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 610, in forward
    layer_outputs = layer_module(
  File "/home/zhangxinyu/.conda/envs/torch19/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zhangxinyu/.conda/envs/torch19/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 496, in forward
    self_attention_outputs = self.attention(
  File "/home/zhangxinyu/.conda/envs/torch19/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zhangxinyu/.conda/envs/torch19/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 426, in forward
    self_outputs = self.self(
  File "/home/zhangxinyu/.conda/envs/torch19/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zhangxinyu/.conda/envs/torch19/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
RuntimeError: CUDA out of memory. Tried to allocate 300.00 MiB (GPU 0; 23.70 GiB total capacity; 21.51 GiB already allocated; 98.81 MiB free; 21.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
