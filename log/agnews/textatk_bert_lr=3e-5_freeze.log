2023-01-17 13:04:52.687470: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-17 13:04:52.878604: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-01-17 13:04:54.337298: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-01-17 13:04:54.337378: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-01-17 13:04:54.337386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-17 13:04:56.446576: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2023-01-17 13:04:56.446616: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
textattack: Loading transformers AutoModelForSequenceClassification
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
textattack: Model state dict:
textattack: bert.embeddings.position_ids	torch.Size([1, 512])
textattack: bert.embeddings.word_embeddings.weight	torch.Size([30522, 768])
textattack: bert.embeddings.position_embeddings.weight	torch.Size([512, 768])
textattack: bert.embeddings.token_type_embeddings.weight	torch.Size([2, 768])
textattack: bert.embeddings.LayerNorm.weight	torch.Size([768])
textattack: bert.embeddings.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.0.attention.self.query.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.0.attention.self.query.bias	torch.Size([768])
textattack: bert.encoder.layer.0.attention.self.key.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.0.attention.self.key.bias	torch.Size([768])
textattack: bert.encoder.layer.0.attention.self.value.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.0.attention.self.value.bias	torch.Size([768])
textattack: bert.encoder.layer.0.attention.output.dense.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.0.attention.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.0.attention.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.0.attention.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.0.intermediate.dense.weight	torch.Size([3072, 768])
textattack: bert.encoder.layer.0.intermediate.dense.bias	torch.Size([3072])
textattack: bert.encoder.layer.0.output.dense.weight	torch.Size([768, 3072])
textattack: bert.encoder.layer.0.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.0.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.0.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.1.attention.self.query.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.1.attention.self.query.bias	torch.Size([768])
textattack: bert.encoder.layer.1.attention.self.key.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.1.attention.self.key.bias	torch.Size([768])
textattack: bert.encoder.layer.1.attention.self.value.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.1.attention.self.value.bias	torch.Size([768])
textattack: bert.encoder.layer.1.attention.output.dense.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.1.attention.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.1.attention.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.1.attention.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.1.intermediate.dense.weight	torch.Size([3072, 768])
textattack: bert.encoder.layer.1.intermediate.dense.bias	torch.Size([3072])
textattack: bert.encoder.layer.1.output.dense.weight	torch.Size([768, 3072])
textattack: bert.encoder.layer.1.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.1.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.1.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.2.attention.self.query.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.2.attention.self.query.bias	torch.Size([768])
textattack: bert.encoder.layer.2.attention.self.key.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.2.attention.self.key.bias	torch.Size([768])
textattack: bert.encoder.layer.2.attention.self.value.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.2.attention.self.value.bias	torch.Size([768])
textattack: bert.encoder.layer.2.attention.output.dense.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.2.attention.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.2.attention.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.2.attention.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.2.intermediate.dense.weight	torch.Size([3072, 768])
textattack: bert.encoder.layer.2.intermediate.dense.bias	torch.Size([3072])
textattack: bert.encoder.layer.2.output.dense.weight	torch.Size([768, 3072])
textattack: bert.encoder.layer.2.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.2.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.2.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.3.attention.self.query.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.3.attention.self.query.bias	torch.Size([768])
textattack: bert.encoder.layer.3.attention.self.key.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.3.attention.self.key.bias	torch.Size([768])
textattack: bert.encoder.layer.3.attention.self.value.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.3.attention.self.value.bias	torch.Size([768])
textattack: bert.encoder.layer.3.attention.output.dense.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.3.attention.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.3.attention.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.3.attention.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.3.intermediate.dense.weight	torch.Size([3072, 768])
textattack: bert.encoder.layer.3.intermediate.dense.bias	torch.Size([3072])
textattack: bert.encoder.layer.3.output.dense.weight	torch.Size([768, 3072])
textattack: bert.encoder.layer.3.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.3.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.3.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.4.attention.self.query.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.4.attention.self.query.bias	torch.Size([768])
textattack: bert.encoder.layer.4.attention.self.key.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.4.attention.self.key.bias	torch.Size([768])
textattack: bert.encoder.layer.4.attention.self.value.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.4.attention.self.value.bias	torch.Size([768])
textattack: bert.encoder.layer.4.attention.output.dense.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.4.attention.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.4.attention.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.4.attention.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.4.intermediate.dense.weight	torch.Size([3072, 768])
textattack: bert.encoder.layer.4.intermediate.dense.bias	torch.Size([3072])
textattack: bert.encoder.layer.4.output.dense.weight	torch.Size([768, 3072])
textattack: bert.encoder.layer.4.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.4.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.4.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.5.attention.self.query.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.5.attention.self.query.bias	torch.Size([768])
textattack: bert.encoder.layer.5.attention.self.key.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.5.attention.self.key.bias	torch.Size([768])
textattack: bert.encoder.layer.5.attention.self.value.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.5.attention.self.value.bias	torch.Size([768])
textattack: bert.encoder.layer.5.attention.output.dense.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.5.attention.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.5.attention.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.5.attention.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.5.intermediate.dense.weight	torch.Size([3072, 768])
textattack: bert.encoder.layer.5.intermediate.dense.bias	torch.Size([3072])
textattack: bert.encoder.layer.5.output.dense.weight	torch.Size([768, 3072])
textattack: bert.encoder.layer.5.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.5.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.5.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.6.attention.self.query.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.6.attention.self.query.bias	torch.Size([768])
textattack: bert.encoder.layer.6.attention.self.key.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.6.attention.self.key.bias	torch.Size([768])
textattack: bert.encoder.layer.6.attention.self.value.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.6.attention.self.value.bias	torch.Size([768])
textattack: bert.encoder.layer.6.attention.output.dense.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.6.attention.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.6.attention.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.6.attention.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.6.intermediate.dense.weight	torch.Size([3072, 768])
textattack: bert.encoder.layer.6.intermediate.dense.bias	torch.Size([3072])
textattack: bert.encoder.layer.6.output.dense.weight	torch.Size([768, 3072])
textattack: bert.encoder.layer.6.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.6.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.6.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.7.attention.self.query.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.7.attention.self.query.bias	torch.Size([768])
textattack: bert.encoder.layer.7.attention.self.key.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.7.attention.self.key.bias	torch.Size([768])
textattack: bert.encoder.layer.7.attention.self.value.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.7.attention.self.value.bias	torch.Size([768])
textattack: bert.encoder.layer.7.attention.output.dense.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.7.attention.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.7.attention.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.7.attention.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.7.intermediate.dense.weight	torch.Size([3072, 768])
textattack: bert.encoder.layer.7.intermediate.dense.bias	torch.Size([3072])
textattack: bert.encoder.layer.7.output.dense.weight	torch.Size([768, 3072])
textattack: bert.encoder.layer.7.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.7.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.7.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.8.attention.self.query.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.8.attention.self.query.bias	torch.Size([768])
textattack: bert.encoder.layer.8.attention.self.key.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.8.attention.self.key.bias	torch.Size([768])
textattack: bert.encoder.layer.8.attention.self.value.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.8.attention.self.value.bias	torch.Size([768])
textattack: bert.encoder.layer.8.attention.output.dense.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.8.attention.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.8.attention.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.8.attention.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.8.intermediate.dense.weight	torch.Size([3072, 768])
textattack: bert.encoder.layer.8.intermediate.dense.bias	torch.Size([3072])
textattack: bert.encoder.layer.8.output.dense.weight	torch.Size([768, 3072])
textattack: bert.encoder.layer.8.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.8.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.8.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.9.attention.self.query.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.9.attention.self.query.bias	torch.Size([768])
textattack: bert.encoder.layer.9.attention.self.key.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.9.attention.self.key.bias	torch.Size([768])
textattack: bert.encoder.layer.9.attention.self.value.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.9.attention.self.value.bias	torch.Size([768])
textattack: bert.encoder.layer.9.attention.output.dense.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.9.attention.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.9.attention.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.9.attention.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.9.intermediate.dense.weight	torch.Size([3072, 768])
textattack: bert.encoder.layer.9.intermediate.dense.bias	torch.Size([3072])
textattack: bert.encoder.layer.9.output.dense.weight	torch.Size([768, 3072])
textattack: bert.encoder.layer.9.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.9.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.9.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.10.attention.self.query.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.10.attention.self.query.bias	torch.Size([768])
textattack: bert.encoder.layer.10.attention.self.key.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.10.attention.self.key.bias	torch.Size([768])
textattack: bert.encoder.layer.10.attention.self.value.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.10.attention.self.value.bias	torch.Size([768])
textattack: bert.encoder.layer.10.attention.output.dense.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.10.attention.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.10.attention.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.10.attention.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.10.intermediate.dense.weight	torch.Size([3072, 768])
textattack: bert.encoder.layer.10.intermediate.dense.bias	torch.Size([3072])
textattack: bert.encoder.layer.10.output.dense.weight	torch.Size([768, 3072])
textattack: bert.encoder.layer.10.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.10.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.10.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.11.attention.self.query.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.11.attention.self.query.bias	torch.Size([768])
textattack: bert.encoder.layer.11.attention.self.key.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.11.attention.self.key.bias	torch.Size([768])
textattack: bert.encoder.layer.11.attention.self.value.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.11.attention.self.value.bias	torch.Size([768])
textattack: bert.encoder.layer.11.attention.output.dense.weight	torch.Size([768, 768])
textattack: bert.encoder.layer.11.attention.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.11.attention.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.11.attention.output.LayerNorm.bias	torch.Size([768])
textattack: bert.encoder.layer.11.intermediate.dense.weight	torch.Size([3072, 768])
textattack: bert.encoder.layer.11.intermediate.dense.bias	torch.Size([3072])
textattack: bert.encoder.layer.11.output.dense.weight	torch.Size([768, 3072])
textattack: bert.encoder.layer.11.output.dense.bias	torch.Size([768])
textattack: bert.encoder.layer.11.output.LayerNorm.weight	torch.Size([768])
textattack: bert.encoder.layer.11.output.LayerNorm.bias	torch.Size([768])
textattack: bert.pooler.dense.weight	torch.Size([768, 768])
textattack: bert.pooler.dense.bias	torch.Size([768])
textattack: classifier.weight	torch.Size([4, 768])
textattack: classifier.bias	torch.Size([4])
textattack: Num params: 85648132
textattack: train set length: 114000
textattack: val set length: 6000
textattack: test set length: 7600
textattack: Writing logs to /data/xinyu/results/fgws/models/bert/agnews//textatk_lr=3e-05test/train_log.txt.
textattack: Wrote original training args to /data/xinyu/results/fgws/models/bert/agnews//textatk_lr=3e-05test/training_args.json.
/home/zhangxinyu/.conda/envs/torch19/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
textattack: ***** Running training *****
textattack:   Num examples = 114000
textattack:   Num epochs = 10
textattack:   Num clean epochs = 10
textattack:   Instantaneous batch size per device = 32
textattack:   Total train batch size (w. parallel, distributed & accumulation) = 32
textattack:   Gradient accumulation steps = 1
textattack:   Total optimization steps = 35630
textattack: ==========================================================
textattack: Epoch 1
textattack: Running clean epoch 1/10
Loss 0.27397: 100%|█████████████████████████| 3563/3563 [04:40<00:00, 12.70it/s]
textattack: Train accuracy: 90.38%
textattack: Eval accuracy: 93.68%
textattack: Best score found. Saved model to /data/xinyu/results/fgws/models/bert/agnews//textatk_lr=3e-05test//best_model/
/home/zhangxinyu/.conda/envs/torch19/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
textattack: ==========================================================
textattack: Epoch 2
textattack: Running clean epoch 2/10
Loss 0.21518: 100%|█████████████████████████| 3563/3563 [04:05<00:00, 14.49it/s]
textattack: Train accuracy: 94.63%
textattack: Eval accuracy: 93.97%
textattack: Best score found. Saved model to /data/xinyu/results/fgws/models/bert/agnews//textatk_lr=3e-05test//best_model/
textattack: ==========================================================
textattack: Epoch 3
textattack: Running clean epoch 3/10
Loss 0.18054: 100%|█████████████████████████| 3563/3563 [04:17<00:00, 13.82it/s]
textattack: Train accuracy: 96.14%
textattack: Eval accuracy: 93.85%
textattack: ==========================================================
textattack: Epoch 4
textattack: Running clean epoch 4/10
Loss 0.15536: 100%|█████████████████████████| 3563/3563 [11:00<00:00,  5.40it/s]
textattack: Train accuracy: 97.26%
textattack: Eval accuracy: 93.65%
textattack: ==========================================================
textattack: Epoch 5
textattack: Running clean epoch 5/10
Loss 0.13665: 100%|█████████████████████████| 3563/3563 [12:55<00:00,  4.59it/s]
textattack: Train accuracy: 97.88%
textattack: Eval accuracy: 93.48%
textattack: ==========================================================
textattack: Epoch 6
textattack: Running clean epoch 6/10
Loss 0.12186: 100%|█████████████████████████| 3563/3563 [12:55<00:00,  4.59it/s]
textattack: Train accuracy: 98.37%
textattack: Eval accuracy: 93.32%
textattack: ==========================================================
textattack: Epoch 7
textattack: Running clean epoch 7/10
Loss 0.11036: 100%|█████████████████████████| 3563/3563 [12:59<00:00,  4.57it/s]
textattack: Train accuracy: 98.55%
textattack: Eval accuracy: 93.32%
textattack: ==========================================================
textattack: Epoch 8
textattack: Running clean epoch 8/10
Loss 0.10090: 100%|█████████████████████████| 3563/3563 [13:02<00:00,  4.56it/s]
textattack: Train accuracy: 98.82%
textattack: Eval accuracy: 93.10%
textattack: ==========================================================
textattack: Epoch 9
textattack: Running clean epoch 9/10
Loss 0.09316: 100%|█████████████████████████| 3563/3563 [13:02<00:00,  4.55it/s]
textattack: Train accuracy: 98.92%
textattack: Eval accuracy: 93.38%
textattack: ==========================================================
textattack: Epoch 10
textattack: Running clean epoch 10/10
Loss 0.08672: 100%|█████████████████████████| 3563/3563 [13:01<00:00,  4.56it/s]
textattack: Train accuracy: 99.01%
textattack: Eval accuracy: 93.22%
textattack: Wrote README to /data/xinyu/results/fgws/models/bert/agnews//textatk_lr=3e-05test/README.md.
textattack: Eval accuracy: 93.68%